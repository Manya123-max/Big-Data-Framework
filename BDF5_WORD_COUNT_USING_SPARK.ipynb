{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNadw/2Vsr2yCiwoRT/8ZKU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manya123-max/Big-Data-Framework/blob/main/BDF5_WORD_COUNT_USING_SPARK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:**\n",
        "The primary aim of this code is to perform a word count analysis on a text file using Apache Spark."
      ],
      "metadata": {
        "id": "iKjA0_l_JHjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Import Libraries\n",
        "\n",
        "The necessary PySpark libraries are imported to create a Spark session and perform DataFrame transformations. Specifically:\n",
        "\n"
      ],
      "metadata": {
        "id": "0Q_ID4Q_K4bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, col"
      ],
      "metadata": {
        "id": "yA0CQJGO_S3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2**: SparkSession\n",
        "\n",
        "It is used to create the Spark application.\n",
        "Functions like explode, split, and col are utilized for text preprocessing and transformations."
      ],
      "metadata": {
        "id": "QukUWBoULEsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Word Count Example\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "y7HD0qEP_Ury"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3**: Load Text Data\n",
        "\n",
        "The text file is loaded into a Spark DataFrame where each line is represented as a single record under the column \"value.\"\n",
        "\n",
        "This is a distributed read operation, which makes it scalable for larger files."
      ],
      "metadata": {
        "id": "s_gbih6zLaJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Text Data\n",
        "# Replace 'path/to/textfile.txt' with the actual path to your text file\n",
        "file_path = \"/content/NLP_adventure_of_sherlock_holmes.txt\"\n",
        "text_df = spark.read.text(file_path)"
      ],
      "metadata": {
        "id": "ZrDzt2fL_XUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4**: Display the Original Text\n",
        "\n",
        "Displays the first five rows of the text file for preview. truncate=False ensures that long lines are not truncated in the output."
      ],
      "metadata": {
        "id": "4KUE4OFGLtFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the Loaded Text\n",
        "print(\"Original Text Data:\")\n",
        "text_df.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaIE0aIm_Z7z",
        "outputId": "6ff61e13-c6c5-4489-96b5-93316bd6a2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text Data:\n",
            "+-----------------------+\n",
            "|value                  |\n",
            "+-----------------------+\n",
            "|                       |\n",
            "|I. A SCANDAL IN BOHEMIA|\n",
            "|                       |\n",
            "|                       |\n",
            "|I.                     |\n",
            "+-----------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5**: Split Text into Words\n",
        "\n",
        "The split function splits each line into words using the regular expression \\\\s+, which matches one or more whitespace characters.\n",
        "\n",
        "The explode function flattens the resulting list of words into individual rows.\n",
        "Each word is given the alias \"word\" for readability."
      ],
      "metadata": {
        "id": "zUdWCfxRL96D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Text into Words\n",
        "words_df = text_df.select(explode(split(col(\"value\"), \"\\\\s+\")).alias(\"word\"))"
      ],
      "metadata": {
        "id": "eGC0kgzc_cf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6:** Count Words\n",
        "\n",
        "The groupBy function groups the data by each unique word.\n",
        "\n",
        "The count function computes the frequency of each word.\n",
        "\n",
        "The resulting DataFrame is ordered in descending order of word count using orderBy.\n"
      ],
      "metadata": {
        "id": "sFqgBB8VMSzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count Words\n",
        "word_count_df = words_df.groupBy(\"word\").count().orderBy(col(\"count\").desc())"
      ],
      "metadata": {
        "id": "8RiKkLLi_ehw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Display Word Counts\n",
        "\n",
        "Displays the top 10 words along with their counts for analysis."
      ],
      "metadata": {
        "id": "FGYlhlE7M06t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Word Counts\n",
        "print(\"Word Counts:\")\n",
        "word_count_df.show(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGnmuOw4_gjl",
        "outputId": "4f27902b-3c8b-4eaf-dc86-68676b73cf07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Counts:\n",
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| the| 2144|\n",
            "|    | 1154|\n",
            "| and| 1135|\n",
            "|   a| 1131|\n",
            "|  of| 1114|\n",
            "|  to| 1064|\n",
            "|   I| 1044|\n",
            "|  in|  669|\n",
            "|that|  578|\n",
            "| was|  535|\n",
            "| his|  436|\n",
            "|  is|  422|\n",
            "|  my|  402|\n",
            "|  it|  399|\n",
            "| you|  398|\n",
            "|  he|  378|\n",
            "|have|  357|\n",
            "| had|  328|\n",
            "|with|  320|\n",
            "|  as|  317|\n",
            "+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Save Word Counts to Disk\n",
        "\n",
        "The word counts are saved in CSV format to the specified directory.\n",
        "\n",
        "mode(\"overwrite\") ensures that any existing data in the output directory is replaced.\n",
        "\n",
        "header=True adds column names to the CSV file."
      ],
      "metadata": {
        "id": "YyBUrhGANDR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Word Counts to Disk\n",
        "output_path = \"path/to/output/directory\"\n",
        "word_count_df.write.mode(\"overwrite\").csv(output_path, header=True) # Added mode=\"overwrite\"\n"
      ],
      "metadata": {
        "id": "RXjshbhZ_jGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Stop the SparkSession\n",
        "\n",
        "Stops the Spark application to free up resources."
      ],
      "metadata": {
        "id": "fwuzlJjlNRUx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKsfmuIo-Ox4"
      },
      "outputs": [],
      "source": [
        "# Stop the SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULT:** This code efficiently processes large datasets by distributing computation across the\n",
        "cluster, showcasing the power of Spark's parallelism"
      ],
      "metadata": {
        "id": "MA6O3C0SNboO"
      }
    }
  ]
}